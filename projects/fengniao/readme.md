# 项目背景
因为朋友说想抓企业的信息，所以就做了个 demo，起初是想用 requests 简单弄下，压根没打算用 scrapy 因为是个小东西，没必要上大家伙，requests 写起来舒服些。谁曾想公司的网络似乎有点问题，公司网络有代理，也尝试配了代理，但是配代理后只能访问公司内网，不清楚是什么原因。想到自己的浏览器可以正常访问外网，所以就自然而然想起了 selenium，虽然好久没有写了，但是凭着曾经的记忆稍微看下就很快上手了，不过这次重新使用 selenium 也有了新的发现，有一些 API 更新了，旧的被废弃了，跑的时候有警告。

# 如何使用
1. 首先需要注册 风鸟 的账号，在代码中找到 user_name.send_keys，user_password.send_keys，填写用户名和密码
2. 修改 csv 文件输出路径，在代码中找到 out_filepath，自行修改即可，如果是当前路径下输出，直接 out_filepath = "out.csv" 即可
3. 是否需要日志文件，如果需要的话，自行配置日志路径，如果不需要在代码中找到 MyLog("../out/requests.log") 改为 MyLog() 即可
4. 错误链接文件 在代码中找到 with open("../out/error_url.log", "a+", encoding='utf-8') as f，修改错误链接文件记录路径，如果不需要删除这两行 （这里其实在第三的 log 中有，但是 第三步的 log 中还含有其他的内容，我懒得写个正则去提取，怎么方便怎么来，索性直接放到一个文件中，后续如果需要处理提取失败的网页可以直接使用）
5. 如果需要观察浏览器删除这行代码：options.add_argument("--headless")，默认无头模式，也就是看不到浏览器显示。
6. 自行下载 edge 对应的 webdriver，根据自己 driver 的路径修改：service = Service(r'D:\msedgedriver.exe')

# 说明
风鸟每个用户每天可以有 100 次的查看机会，所以要么充钱，要么多账户，如果充钱也就没必要写这个爬虫了，因为充钱后支持自动导出数据，所以可以采用多账户。如果有 10 个以上的账户我会考虑多账户逻辑，但是问题是没有，所以我直接手改了。

不知道是不是风鸟故意的，还是怎么的，企业信息页面似乎有两种模板，我只写了一种，另外一种数目不多，这东西其实都是看需求，有的话就写一下也不难，没有的话就算了。

最开心的当让是开始注意到 魔鬼数字，在今后的代码中，我都将尽量消除 魔鬼数字，除了 0 和 1 代码中不应该出现其他的数字。

还有一个需要注意的地方就是 is_write_header，当第一次创建 csv 文件的时候，我们应该将 is_write_header = True，如果程序是多账户一次跑，那么 is_write_header = True，如果程序是分次数跑，那么在创建 csv 文件的时候 is_write_header = True，后面就改为 False

# 综述
看起来是个很简单的爬虫但是却不简单，好在网站没有验证码登录什么的，否则还得加上图像识别的库来处理，会麻烦一些

用到的库：

1. selenium
2. easydict
3. csv
4. logging

涉及到的 xpath js 对于初次接触的人需要一段时间消化
